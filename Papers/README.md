# Papers #
The "Papers" subdirectory is used for consolidation of the papers read in order to prepare for this project. The first section of papers are in relation to the boosting ensemble method, particularly XGBoost (Extreme Gradient Boosting). The second references other papers used for the research extension portion of the project, primarily tailored to deep neural network models. The below sections are as follows: a Google Drive for a consolidation of the papers, individual sources of each paper, and a summary of each paper as well as the relevancy they have. As of writing, there are a total of six papers which may increase with the continuation of the project.

## Sources ##
### XGBoost ###
A Comparative Analysis of XGBoost: https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10462-020-09896-5&casa_token=v9KfN1RU_r4AAAAA:wDzWFvq66KPK4VBnPBXhjZZLnrr-bxQqD1xUSPQrbZ0V_dGnawRmgITRX_Rg_UFoqYp5j1yReLILV3Cz

Diabetes Prediction Based on XGBoost Algorithm: https://iopscience.iop.org/article/10.1088/1757-899X/768/7/072093/pdf

Imbalance-XGBoost: leveraging weighted and focal losses for binary label-imbalanced classification with XGBoost: https://www.sciencedirect.com/science/article/pii/S0167865520302129?casa_token=1pqmPhixTewAAAAA:i9MEY__UA2l7P2WMejRHBnioCAdSf3uRohF-5sGnaeUv9LtvzG6B87DzLPB2ET9vECFrRAdNuA

XGBoost: A Scalable Tree Boosting System: https://dl.acm.org/doi/pdf/10.1145/2939672.2939785

### Deep Neural Network ###

An Efficient XGBooost-DNN-Based Classification Model for Network Intrusion Detection System: https://link.springer.com/article/10.1007/s00521-020-04708-x

Traing Deep Neural Networks on Imbalanced Data Sets: http://203.170.84.89/~idawis33/DataScienceLab/publication/IJCNN15.wang.final.pdf

## Summaries ##
### A Comparative Analysis of XGBoost ###
The paper provides a comparison between XGBoost, gradient boosting, and random forest algorithms in terms of their performance, parameterization, and generalization across various datasets. XGBoost, based on gradient boosting, is considered a scalable and efficient technique for machine learning challenges, often competing in accuracy and training speed, especially when tuned properly. The study demonstrates that even though gradient boosting performs slightly better overall, the differences in performance among tuned XGBoost, default gradient boosting, and random forest are not statistically significant. The paper continues to express the importance of parameter tuning and how it is crucial for accurate models based on gradient boosting, while random forests tend to perform well with default parameters. The experiments conducted suggest that tuning certain parameters in XGBoost, specifically randomization parameters like subsampling rate and the number of features per split, might not significantly impact performance. A fixed subsampling rate of 0.75 without replacement and using the square root of features at each split can streamline the parameter grid search process for XGBoost, improving its average performance. The study also highlights XGBoost's advantage in enabling parameter tuning with a computationally efficient algorithm compared to the more resource-intensive process required for gradient boosting or random forest. This comprehensive analysis underscores the importance of parameter optimization for boosting-based models while showing the efficiency and competitiveness of XGBoost in handling diverse machine learning tasks.

### Diabetes Prediction Based on XGBoost Algorithm ###
The literature focuses on using data mining and machine learning techniques to understand significant attributes of diabetes, aiming to predict and prevent the condition. Using XGBoost, the paper presents a prediction algorithm which separates numerical features and extracts elements from textual data. The model achieves an accuracy of 80.2% in diabetes prediction. While various algorithms like XGBoost, random forest, and combined clustering-XGBoost for heart disease prediction have been employed, other methods like SVM, Decision Trees, Naive Bayes, Logistic Regression, and BP Neural Networks have shown limitations. The paper proposes an iterative calculation of weak classifiers using XGBoost and data preprocessing on experimental data from the National Institute of Diabetes and Digestive and Kidney Diseases to establish an accurate classification model. By denoising, feature extraction, and preprocessing techniques such as null value handling, outlier processing, and correlation analysis, the study shows the significance of accurate data preprocessing for model efficiency. XGBoost emerges as the most effective among the tested models, in terms of higher accuracy, faster training times, and efficient learning of feature weights. The feature combination technique, employing iterative methods to derive subsets, enhances diabetes prediction accuracy, stability, and computational speed. The paper emphasizes the role of data preprocessing, highlights XGBoost's efficiency over other models, and proposes a feature combination method in achieving high prediction accuracy for diabetes.

### Imbalance-XGBoost: leveraging weighted and focal losses for binary label-imbalanced classification with XGBoost ###
The article introduces Imbalance-XGBoost, a Python-based package designed to address label-imbalanced binary classification tasks using XGBoost. By integrating weighted cross-entropy and focal loss functions, it aims to mitigate the challenges issued by imbalanced datasets. The implementation involves specialized classes for the loss functions, enabling parameter tuning and model fitting via the usual fit() function. The paper delves into the algebraic derivation of first- and second-order derivatives used for incorporating the loss functions into the XGBoost framework. Evaluation is conducted on Parkinson's disease data and benchmark datasets from the UCI repository showcasing competitive performances through ROC and Precision-Recall curves, demonstrating high F1 scores for both weighted and focal loss methods compared to the regular XGBoost algorithm. The experiments show a slight accuracy decline but again, higher F1 scores, especially for focal-XGBoost across many feature groups. The package's contributions are in providing a practical solution that combines XGBoost with effective imbalance loss functions, establishing the perspective of handling imbalanced data for enhanced classification accuracy.

### XGBoost: A Scalable Tree Boosting System ###
The paper offers a comprehensive overview of XGBoost, a highly scalable tree boosting system that excels in handling sparse data and achieving competitive results across various applications. XGBoost's key features include weighted quantile sketch for approximate tree learning, cache access patterns, data compression, and sharding techniques, which contribute to building a scalable end-to-end tree boosting system. The system's most important development involves efficient algorithms for handling sparse data, the introduction of theoretical methodologies for weighted quantile sketching, and the implementation of sparsity-aware algorithms for parallel tree learning. XGBoost uses techniques such as shrinkage, column subsampling, and cache-aware access to prevent overfitting, increase computation speed, and optimize memory usage. The design of the system involves column blocks for parallel learning, cache-aware access, block compression, and sharding to be able to handle large datasets. Its approach towards various sparsity patterns and with its implementation across different programming languages and distributed systems show its versatility in different environments. The experiments across various datasets and settings demonstrate XGBoost's high performance compared to other existing solutions. XGBoost is shown to be a great solution for robust tree boosting in machine learning due to the innovative algorithms developed, efficient design strategies, and scalability.

### An Efficient XGBooost-DNN-Based Classification Model for Network Intrusion Detection System ###
The paper proposes a novel XGBoostâ€“DNN model has been claimed for effective intrusion detection in network security. The model is comprised of the main steps: normalization, feature selection using XGBoost, and classification via deep neural networks (DNN). Employing the NSL-KDD dataset and tools (TensorFlow & Python), the model's results outperform traditional shallow learning methods (e.g., logistic regression, SVM, naive Bayes) in terms of accuracy, precision, recall, and F1-score. The proposed model is validated via cross-validation which shows improved intrusion detection capabilities compared to existing methodologies, highlighting the advantages of deep neural networks and the XGBoost technique in network security applications using the NSL-KDD dataset.


### Traing Deep Neural Networks on Imbalanced Data Sets ### 
The paper addresses the issue of class imbalance in deep learning by proposing novel loss functions, Mean False Error (MFE) and Mean Squared False Error (MSFE), to enhance sensitivity to minority classes and improve accuracy on imbalanced data. It highlights the downfalls of some classifiers in handling imbalanced data and emphasizes the lack of consideration for imbalance in existing deep learning algorithms. The loss functions aim to mitigate bias towards majority classes observed in the loss function Mean Square Error (MSE). The function does so by considering errors from both majority and minority classes as equals. The empirical evaluations involve diverse datasets such as images from CIFAR-100 and documents from 20 Newsgroups. The authors validate the efficacy of MFE and MSFE by demonstrating consistent outperformance over MSE, particularly on severely imbalanced datasets. The paper's contributions allow for theoretical analysis, empirical validation, and practical insights into addressing class imbalance in primarily deep neural networks, to give a better approach to enhance classification accuracy on imbalanced data.